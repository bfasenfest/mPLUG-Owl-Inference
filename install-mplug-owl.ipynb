{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Repo\n",
    "!git clone https://github.com/X-PLUG/mPLUG-Owl.git\n",
    "%cd mPLUG-Owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Conda environment\n",
    "%pip install pytorch torchvision==0.14.1 torchaudio==0.13.1\n",
    "%pip install -r requirements.txt\n",
    "%pip install protobuf==3.20.*\n",
    "\n",
    "# Install nvidia_smi for logging VRAM if needed\n",
    "%pip install nvidia_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, video inference needs to use a custom model instead of the HF model\n",
    "!mkdir model\n",
    "%cd model\n",
    "!git clone https://huggingface.co/MAGAer13/mplug-owl-llama-7b-video\n",
    "!rm pytorch_model.bin\n",
    "!wget \"http://mm-chatgpt.oss-cn-zhangjiakou.aliyuncs.com/mplug_owl_demo/released_checkpoint/pytorch_model.bin\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir videos\n",
    "# Add videos you want to caption to this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mplug-owl-inference.py "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
